{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b74a4043-4239-435b-9b0a-9649b2713964",
   "metadata": {},
   "source": [
    "# Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22de4ca5-4950-4ce9-bd14-3fc970c23ab1",
   "metadata": {},
   "source": [
    "In this notebook I will explain the process I made to create `extractor.py` as well as the other .py that are used there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d18e7b-ff40-4cdd-ac09-f85a1abd74af",
   "metadata": {},
   "source": [
    "The analyzers would be preloaded on another .py called `analyzer.py` and it would look like this. The reason for it to be on another .py is because they take some time to load, so having them pre-loaded will save a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84e34710-f694-4ade-88f3-8c396e1acbf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pysentimiento import create_analyzer\n",
    "\n",
    "analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
    "\n",
    "hate_speech_analyzer = create_analyzer(task=\"context_hate_speech\", lang=\"es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ae0cf1-9792-4351-ba8b-29b4c2d8c1bb",
   "metadata": {},
   "source": [
    "Then we will load the options for the driver. This doesn't take any time to load but it would be on another .py called `driver_options.py` in order to make `extractor.py` cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a16af04-b16e-49a7-8e46-4c900fb32879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "opciones=Options()\n",
    "\n",
    "opciones.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "opciones.add_experimental_option('useAutomationExtension', False)\n",
    "opciones.headless=False\n",
    "opciones.add_argument('--start-maximized')\n",
    "opciones.add_argument('--incognito')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612976aa-aa78-40b4-a008-b74a402426d5",
   "metadata": {},
   "source": [
    "This is the first part of the extractor in which we will extract the top 5 streamers we want to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2490a953-8cfa-4fcd-8f59-f3c378e246c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ibai', 'lla', 'lvpes', 'nickdaboom', 'xownz']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "\n",
    "# Driver gets initialize and we give it time to load\n",
    "driver = webdriver.Chrome(opciones)\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "url = 'https://twitchtracker.com/channels/live/spanish' # web with top live streams in spanish at the moment\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# process to get the top_5\n",
    "table = driver.find_element(By.CSS_SELECTOR, 'table')\n",
    "top_5 = table.find_elements(By.CSS_SELECTOR, 'tr')[:5]\n",
    "\n",
    "users = []\n",
    "\n",
    "for e in top_5:\n",
    "    users.append(e.find_elements(By.CSS_SELECTOR, 'a')[1].text.lower())\n",
    "\n",
    "driver.quit()\n",
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a47c3-8cf6-44e5-9f73-9347d4925a36",
   "metadata": {},
   "source": [
    "Now the user would be feed to the pipeline. This would be ideally done with JobLib's Parallel function, but the machine with which I'm running the code is not powerful enough. Alas, at the end of the notebook the code for using multiple processes with JobLib would be provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51733e9c-130d-4ba7-ad56-cce8d2424509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "from chat_downloader import ChatDownloader # library for live chat data scraping\n",
    "\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from passwords import STR_CONN # a file with my connection string to the mongodb atlas db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08af93ad-6667-4f3d-ac34-bcf760cc636a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extractor(user):\n",
    "\n",
    "    url = f'https://www.twitch.tv/{user}'\n",
    "    chat = ChatDownloader().get_chat(url,\n",
    "                                    retry_timeout = -1, # -1 makes the downloader to retreive a message as soon as is published\n",
    "                                    timeout = 150)      # 150 secs of scrapping\n",
    "    temp = []\n",
    "    for message in chat:                        \n",
    "        temp.append(message)\n",
    "        \n",
    "    # load mongo cursor\n",
    "    cursor = MongoClient(STR_CONN)\n",
    "\n",
    "    db = cursor.live_chats\n",
    "\n",
    "    # video data\n",
    "    vid_id = chat.__dict__['id']\n",
    "    vid_url = url\n",
    "    vid_title = chat.__dict__['title']\n",
    "\n",
    "    video_son = {\n",
    "                '_id': vid_id,\n",
    "                'title': vid_title,\n",
    "                'first_recored': datetime.datetime.now(),\n",
    "                'last_update': datetime.datetime.now()\n",
    "                }\n",
    "\n",
    "    # try pass just in case the author id is already there\n",
    "    try:\n",
    "        db.video.insert_one(video_son)\n",
    "    except:\n",
    "        db.video.update_one(\n",
    "                            {\"_id\": vid_id},\n",
    "                            {\"$set\": {\"last_update\": datetime.datetime.now()}}\n",
    "                            )\n",
    "    # message and author\n",
    "    messages = []\n",
    "    commentors = []\n",
    "\n",
    "    for samp in temp:\n",
    "        # message\n",
    "        mess = samp['message']\n",
    "        mess_id = samp['message_id']\n",
    "        sent = analyzer.predict(mess).__dict__['output']\n",
    "        hate = hate_speech_analyzer.predict(mess).__dict__['output']\n",
    "        # common\n",
    "        ts = samp['timestamp']\n",
    "        # author\n",
    "        name = samp['author']['name']\n",
    "        com_id = samp['author']['id']\n",
    "\n",
    "        mess_son = {\n",
    "                    '_id': mess_id,\n",
    "                    'message': mess,\n",
    "                    'date': datetime.datetime.now(),\n",
    "                    'timestamp': ts,\n",
    "                    'commentator_id': com_id,\n",
    "                    'video_id': vid_id,\n",
    "                    'sentiment_analysis': sent,\n",
    "                    'hat_speech_analysis': hate\n",
    "                    }\n",
    "\n",
    "        auth_son = {\n",
    "                    '_id': com_id,\n",
    "                    'name': name,\n",
    "                    'last_update': datetime.datetime.now()\n",
    "                    }\n",
    "\n",
    "        messages.append(mess_son)\n",
    "        commentors.append(auth_son)\n",
    "        \n",
    "    # sometimes, a message can be recorded twice, this will prevent any error\n",
    "    for message in messages:\n",
    "        try:\n",
    "            db.message.insert_one(message)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # just in case a commentor is a recurrent user, we will update the last_update\n",
    "    for commentor in commentors:\n",
    "        try:\n",
    "            db.user.insert_one(commentor)\n",
    "        except:\n",
    "            db.user.update_one(\n",
    "                    {\"_id\": commentor['_id']},\n",
    "                    {\"$set\": {\"last_update\": datetime.datetime.now()}}\n",
    "                    )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba77504-ffe7-4efe-a707-35e3df7b6cd8",
   "metadata": {},
   "source": [
    "And finally we will test it with only `user[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c00418d0-2487-48c1-8f6c-aa71bdf37cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user = users[0]\n",
    "\n",
    "extractor(user)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironproject",
   "language": "python",
   "name": "ironproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
